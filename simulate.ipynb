{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "17"
    }
   },
   "outputs": [],
   "source": [
    "%run algorithms.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "3"
    }
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from multiprocessing.managers import BaseManager\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "11"
    }
   },
   "outputs": [],
   "source": [
    "class SharedState(object):\n",
    "  def __init__(self, slot_count=None, level_count_per_slot=None, additive=None):\n",
    "    self.regret_shared_array_base = multiprocessing.Array(ctypes.c_double, simulation_count * horizon)\n",
    "    self.regret = np.ctypeslib.as_array(self.regret_shared_array_base.get_obj())\n",
    "    self.regret = self.regret.reshape((simulation_count, horizon))\n",
    "     \n",
    "    self.reward_shared_array_base = multiprocessing.Array(ctypes.c_double, simulation_count * horizon)\n",
    "    self.reward = np.ctypeslib.as_array(self.reward_shared_array_base.get_obj())\n",
    "    self.reward = self.reward.reshape((simulation_count, horizon))\n",
    "    \n",
    "    if slot_count is None or level_count_per_slot is None:\n",
    "      # Fixed world.\n",
    "      slot_count = 3\n",
    "      level_count_per_slot = 3\n",
    "      input_marginals = np.array([[0.08, 0.10, 0.09], [0.11, 0.11, 0.06], [0.05, 0.16, 0.07]])\n",
    "      self.worlds = [World(slot_count=slot_count,level_count_per_slot=level_count_per_slot, input_marginals=input_marginals)] \\\n",
    "                     * simulation_count\n",
    "    else:\n",
    "      self.worlds = []\n",
    "      for s in range(simulation_count):\n",
    "        self.worlds.append(World(slot_count=slot_count,level_count_per_slot=level_count_per_slot,\n",
    "                                 additive=additive))\n",
    "      \n",
    "  def get_world(self, s):\n",
    "    return self.worlds[s]\n",
    "  \n",
    "  def update_regret(self, s, regret_per_period):\n",
    "    self.regret[s, :] = regret_per_period[:]\n",
    "   \n",
    "  def get_regret(self):\n",
    "    return self.regret\n",
    "   \n",
    "  def update_reward(self, s, reward_per_period):\n",
    "    self.reward[s, :] = reward_per_period[:]\n",
    "   \n",
    "  def get_reward(self):\n",
    "    return self.reward\n",
    "   \n",
    "BaseManager.register('SharedState', SharedState)\n",
    " \n",
    "\n",
    "def single_simulation(s, agent_type, shared_state):\n",
    "  t0 = time.time()\n",
    "  \n",
    "  world = shared_state.get_world(s)\n",
    "  \n",
    "  if (agent_type == \"IndependentBernoulliArmsTSAgent\"):\n",
    "    agent = IndependentBernoulliArmsTSAgent(world = world, horizon = horizon)\n",
    "  elif (agent_type == \"MarginalPosteriorTSAgent\"):\n",
    "    agent = MarginalPosteriorTSAgent(world = world, horizon = horizon)\n",
    "  elif (agent_type == \"MarginalPosteriorUCBAgent\"):\n",
    "    agent = MarginalPosteriorTSAgent(world = world, horizon = horizon)\n",
    "  elif (agent_type == \"LogisticRegressionTSAgent\"):\n",
    "    agent = LogisticRegressionTSAgent(world = world, horizon = horizon, regularization_parameter = regularization_parameter)\n",
    "  elif (agent_type == \"LogisticRegressionUCBAgent\"):\n",
    "    agent = LogisticRegressionUCBAgent(world = world, horizon = horizon, regularization_parameter = regularization_parameter)\n",
    "  else:\n",
    "    print(\"This agent_type is not supported.\")\n",
    "    return\n",
    "     \n",
    "  agent.run()\n",
    "  shared_state.update_regret(s, agent.regret_per_period)\n",
    "  shared_state.update_reward(s, agent.reward_per_period)\n",
    "   \n",
    "  t1 = time.time()\n",
    "\n",
    "\n",
    "def run(agent_types, output_prefix, slot_count=None, level_count_per_slot=None, additive=None):  \n",
    "  if (seed > 0):\n",
    "    np.random.seed(seed)\n",
    "   \n",
    "  manager = BaseManager()\n",
    "  manager.start()\n",
    "  pool = multiprocessing.Pool(core_count) \n",
    "  shared_state = manager.SharedState(slot_count=slot_count, level_count_per_slot=level_count_per_slot)\n",
    "  \n",
    "  for agent_type in agent_types:\n",
    "    t0 = time.time()\n",
    "    func = partial(single_simulation, agent_type = agent_type, shared_state = shared_state)\n",
    "    pool.map(func, range(simulation_count))\n",
    "    t1 = time.time()\n",
    "    print(\"{} Elapsed Time: {}\".format(agent_type, (t1 - t0)))\n",
    "   \n",
    "    parameters = \"\"\n",
    "    if (agent_type == \"LogisticRegressionTSAgent\" or agent_type == \"LogisticRegressionUCBAgent\"):\n",
    "      parameters = \"_R{}\".format(regularization_parameter)\n",
    "    \n",
    "    if not os.path.exists(\"Results/\"):\n",
    "        os.makedirs(\"Results/\")\n",
    "    filename = \"Results/{}_{}_H{}_S{}{}\".format(output_prefix, agent_type, horizon, simulation_count, parameters)\n",
    "\n",
    "    np.save(filename + \"_Regret.npy\", np.mean(shared_state.get_regret(), axis = 0))\n",
    "    np.save(filename + \"_RegretVar.npy\", np.var(shared_state.get_regret(), axis = 0))\n",
    "    np.save(filename + \"_ET.npy\", (t1-t0) / simulation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "15"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "horizon = 50000\n",
    "simulation_count = 1000\n",
    "regularization_parameter = 10\n",
    "seed = 1704 \n",
    "core_count = multiprocessing.cpu_count()\n",
    "\n",
    "agent_types = [\"IndependentBernoulliArmsTSAgent\", \"MarginalPosteriorTSAgent\", \"LogisticRegressionTSAgent\"]\n",
    "slot_count_list = [2, 3, 4]\n",
    "level_count_per_slot_list = [2, 3, 4, 5]\n",
    "additive_world_type_list = [True, False]\n",
    "for slot_count, level_count_per_slot, additive_world_type in product(slot_count_list, level_count_per_slot_list, \n",
    "                                                                     additive_world_type_list):\n",
    "  output_prefix = \"SLOT{}LEVEL{}_ADD{}\".format(slot_count, level_count_per_slot, additive_world_type)\n",
    "  print(output_prefix)\n",
    "  run(agent_types = agent_types, output_prefix=output_prefix, \n",
    "      slot_count=slot_count, level_count_per_slot=level_count_per_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "18"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nterop": {
   "seedId": "18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
